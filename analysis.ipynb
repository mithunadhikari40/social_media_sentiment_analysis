{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Social Media Data\n",
    "\n",
    "This notebook walks through a complete sentiment analysis project on social media data. We will cover the entire pipeline from data collection to model evaluation and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup: Importing Libraries\n",
    "\n",
    "First, we import all the necessary libraries for data manipulation, text preprocessing, modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer \u001b[38;5;66;03m# Lemmatization to reduce words to their base form\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer \u001b[38;5;66;03m# TF-IDF for text representation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m \u001b[38;5;66;03m# For more advanced text processing\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Machine Learning models\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV \u001b[38;5;66;03m# For splitting data and hyperparameter tuning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/spacy/__init__.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[32m      8\u001b[39m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/spacy/errors.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/spacy/compat.py:39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatalogue\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthinc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     41\u001b[39m pickle = pickle\n\u001b[32m     42\u001b[39m copy_reg = copy_reg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/thinc/api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     CupyOps,\n\u001b[32m      3\u001b[39m     MPSOps,\n\u001b[32m      4\u001b[39m     NumpyOps,\n\u001b[32m      5\u001b[39m     Ops,\n\u001b[32m      6\u001b[39m     get_current_ops,\n\u001b[32m      7\u001b[39m     get_ops,\n\u001b[32m      8\u001b[39m     set_current_ops,\n\u001b[32m      9\u001b[39m     set_gpu_allocator,\n\u001b[32m     10\u001b[39m     use_ops,\n\u001b[32m     11\u001b[39m     use_pytorch_for_gpu_memory,\n\u001b[32m     12\u001b[39m     use_tensorflow_for_gpu_memory,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/thinc/backends/__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_cupy_allocators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_server\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcupy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/thinc/backends/cupy_ops.py:16\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     is_cupy_array,\n\u001b[32m      8\u001b[39m     is_mxnet_gpu_array,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     torch2xp,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@registry\u001b[39m.ops(\u001b[33m\"\u001b[39m\u001b[33mCupyOps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCupyOps\u001b[39;00m(Ops):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/thinc/backends/numpy_ops.pyx:1\u001b[39m, in \u001b[36minit thinc.backends.numpy_ops\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd # Used for creating and manipulating dataframes\n",
    "import numpy as np # Used for numerical operations\n",
    "\n",
    "# Import .env files to load environement varialbles.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Text preprocessing and feature extraction\n",
    "import re # Regular expressions for cleaning text\n",
    "import nltk # Natural Language Toolkit for stopwords\n",
    "from nltk.corpus import stopwords # List of common words to remove\n",
    "from nltk.stem import WordNetLemmatizer # Lemmatization to reduce words to their base form\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDF for text representation\n",
    "import spacy # For more advanced text processing\n",
    "\n",
    "# Machine Learning models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # For splitting data and hyperparameter tuning\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes classifier\n",
    "from sklearn.svm import SVC # Support Vector Machine classifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix # Evaluation metrics\n",
    "\n",
    "# Deep Learning models (using PyTorch)\n",
    "import torch # Main PyTorch library\n",
    "from torch.utils.data import DataLoader, TensorDataset # For handling data in PyTorch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW # BERT model and tokenizer\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt # For creating static plots\n",
    "import seaborn as sns # For more attractive statistical plots\n",
    "from wordcloud import WordCloud # For generating word clouds\n",
    "import plotly.express as px # For interactive plots\n",
    "\n",
    "# Saving models and files\n",
    "import os # For interacting with the operating system (e.g., creating directories)\n",
    "import joblib # For saving and loading scikit-learn models\n",
    "import json # For saving results to a JSON file\n",
    "\n",
    "# Download necessary NLTK data (if not already present)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Collection\n",
    "\n",
    "In this step, we collect the data for our analysis. The ideal source is live data from a social media platform like Twitter. However, since API keys cannot be shared, we provide two options:\n",
    "\n",
    "1.  **Live Data Collection (Placeholder):** Code using `tweepy` to fetch tweets. You would need to insert your own Twitter API credentials to run this.\n",
    "2.  **Sample CSV Fallback:** If API keys are unavailable, we load data from a pre-made CSV file (`sample_tweets.csv`). **This is the default method for this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample data from sample_tweets.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 4, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing sample data from sample_tweets.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Display the first few rows of the dataframe\u001b[39;00m\n\u001b[32m     42\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/education/masters_in_computer_application_throug_mysore_university/last_semester/project/social_media_analysis/venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 3 fields in line 4, saw 5\n"
     ]
    }
   ],
   "source": [
    "# --- How to Modify ---\n",
    "# USE_LIVE_DATA: Set to True to use the live Twitter API, or False to use the sample CSV.\n",
    "# csv_path: The file path for the sample data.\n",
    "# query: The search term for fetching live tweets (e.g., 'Python programming').\n",
    "# max_results: The number of live tweets to fetch (between 10 and 100).\n",
    "\n",
    "USE_LIVE_DATA = False \n",
    "csv_path = 'sample_tweets.csv'\n",
    "query = 'AI ethics'\n",
    "max_results = 100 \n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "\n",
    "if USE_LIVE_DATA:\n",
    "    if not bearer_token or bearer_token == \"YOUR_BEARER_TOKEN_HERE\":\n",
    "        raise ValueError(\"Twitter Bearer Token not found. Please set it in your .env file.\")\n",
    "        \n",
    "    import tweepy\n",
    "    \n",
    "    print(f\"Attempting to fetch {max_results} live tweets for the query: '{query}'...\")\n",
    "    client = tweepy.Client(bearer_token)\n",
    "    response = client.search_recent_tweets(query=query, max_results=max_results)\n",
    "    \n",
    "    tweets_data = []\n",
    "    if response.data:\n",
    "        for tweet in response.data:\n",
    "            # For simplicity, we assign a 'neutral' sentiment to all fetched tweets.\n",
    "            # In a real-world scenario, these would need to be labeled.\n",
    "            tweets_data.append({'tweet': tweet.text, 'sentiment': 'neutral'})\n",
    "        df = pd.DataFrame(tweets_data)\n",
    "        print(f\"Successfully fetched {len(df)} tweets.\")\n",
    "    else:\n",
    "        print(\"No tweets found for the query. Falling back to the sample CSV.\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    print(\"Using sample data from sample_tweets.csv\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Data Visualization: Sentiment Distribution\n",
    "\n",
    "Let's visualize the distribution of sentiments in our dataset. This gives us a first look at the balance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'sentiment' in df.columns:\n",
    "    # Create a figure for the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create a bar plot of sentiment counts\n",
    "    sns.countplot(x='sentiment', data=df, palette='viridis')\n",
    "    \n",
    "    # Add titles and labels for clarity\n",
    "    plt.title('Distribution of Sentiments in the Dataset', fontsize=16)\n",
    "    plt.xlabel('Sentiment', fontsize=12)\n",
    "    plt.ylabel('Number of Tweets', fontsize=12)\n",
    "    \n",
    "    # Save the plot to the 'plots' directory\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots') # Create the directory if it doesn't exist\n",
    "    plt.savefig('plots/sentiment_distribution.png') # Save the figure\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text Representation: Converting Text to Numbers\n",
    "\n",
    "Machine learning models can't understand raw text. We need to convert our cleaned text data into numerical vectors. We will explore two popular methods:\n",
    "\n",
    "1.  **TF-IDF (Term Frequency-Inverse Document Frequency):** A classical feature extraction technique that represents text as a matrix of word counts, weighted by their importance.\n",
    "2.  **Transformer Embeddings (BERT):** A modern technique using deep learning to create dense, context-aware vector representations of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Splitting Data into Training and Testing Sets\n",
    "\n",
    "Before we create text representations, we must split our data. This ensures that our model is evaluated on data it has never seen before, giving us a true measure of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to Modify ---\n",
    "# test_size: The proportion of the dataset to allocate to the test set. A common value is 0.2 (20%). Changing this affects how much data the model trains on vs. is tested on. More training data can lead to a better model, but a smaller test set might give a less reliable evaluation.\n",
    "# random_state: A seed for reproducibility. Setting this ensures you get the same train/test split every time you run the code.\n",
    "\n",
    "if df is not None:\n",
    "    # Define features (X) and target (y)\n",
    "    X = df['cleaned_tweet'] # The cleaned text data\n",
    "    y = df['sentiment']   # The sentiment labels\n",
    "    \n",
    "    # Perform the split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2,      # Use 20% of the data for testing\n",
    "        random_state=42,    # Ensures the split is the same every time\n",
    "        stratify=y          # Ensures the distribution of sentiments is the same in train and test sets\n",
    "    )\n",
    "    \n",
    "    # Print the shapes of the resulting datasets\n",
    "    print(f'Training data shape: {X_train.shape}')\n",
    "    print(f'Testing data shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Method 1: TF-IDF Vectorization\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It evaluates how relevant a word is to a document in a collection of documents.\n",
    "-   **Term Frequency (TF):** How often a word appears in a document.\n",
    "-   **Inverse Document Frequency (IDF):** The inverse of how many documents contain the word. This gives higher weight to rarer words.\n",
    "\n",
    "The TF-IDF score is the product of these two, resulting in a sparse matrix where each row is a tweet and each column is a unique word from our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to Modify ---\n",
    "# ngram_range: The range of n-grams to consider. (1, 1) means only single words (unigrams). (1, 2) means both unigrams and bigrams (two-word phrases). Using bigrams can capture more context but dramatically increases the number of features.\n",
    "# max_features: The maximum number of top features (words) to keep. This helps control the dimensionality of the feature space.\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # Consider both single words and two-word phrases\n",
    "    max_features=5000    # Limit the vocabulary size to the top 5000 features\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Only transform the test data (using the vocabulary from the training data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the shape of the resulting TF-IDF matrices\n",
    "print(f'Shape of TF-IDF training matrix: {X_train_tfidf.shape}')\n",
    "print(f'Shape of TF-IDF testing matrix: {X_test_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Method 2: Transformer Embeddings (BERT)\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art model that generates dense, context-aware embeddings. Unlike TF-IDF, which treats words in isolation, BERT understands the meaning of a word based on its surrounding words.\n",
    "\n",
    "**Key Differences:**\n",
    "| Feature | TF-IDF | BERT Embeddings |\n",
    "|---|---|---|\n",
    "| **Representation** | Sparse matrix (mostly zeros) | Dense vector (meaningful numbers) |\n",
    "| **Context** | No, treats words independently | Yes, understands context (e.g., 'bank' as in river vs. finance) |\n",
    "| **Vector Size** | Large (equals vocabulary size) | Small and fixed (e.g., 768 dimensions) |\n",
    "| **Computational Cost**| Low | High |\n",
    "\n",
    "We will not generate the embeddings separately. Instead, the tokenization and embedding process will be handled directly by the BERT model during the training phase in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the pre-trained BERT tokenizer to prepare the data for the model.\n",
    "# This step is just a demonstration of how the tokenizer works.\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example of tokenizing a single sentence\n",
    "sample_text = \"This is a sample sentence for BERT.\"\n",
    "encoded_sample = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    add_special_tokens=True, # Add '[CLS]' and '[SEP]' tokens\n",
    "    max_length=32,           # Pad or truncate to a fixed length\n",
    "    padding='max_length',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'      # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(\"Input IDs:\", encoded_sample['input_ids'])\n",
    "print(\"Attention Mask:\", encoded_sample['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training\n",
    "\n",
    "Now we will train and evaluate three different models on our preprocessed data. We'll start with two classical machine learning models using the TF-IDF features and then move to a deep learning model using BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Model 1: Multinomial Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem with a 'naive' assumption of conditional independence between features. It's computationally efficient and works well for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to Modify ---\n",
    "# alpha (smoothing parameter): A value added to the numerators for smoothing. It prevents zero probabilities for features not seen in the training data. A higher alpha makes the model less sensitive to the training data.\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb_model = MultinomialNB(alpha=1.0) # alpha=1.0 is a common default (Laplace smoothing)\n",
    "\n",
    "# Train the model on the TF-IDF training data\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f'Naive Bayes Accuracy: {accuracy_nb:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(nb_model, 'models/naive_bayes_model.pkl')\n",
    "print(\"Naive Bayes model saved to models/naive_bayes_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Model 2: Support Vector Machine (SVM)\n",
    "\n",
    "SVMs are powerful classifiers that find the optimal hyperplane separating data points of different classes. They are particularly effective in high-dimensional spaces, like the one created by TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to Modify ---\n",
    "# C (Regularization parameter): Controls the trade-off between achieving a low training error and a low testing error. A high C value gives more weight to the training data, potentially leading to overfitting.\n",
    "# kernel: The type of kernel to use. 'linear' is often a good start for text data. 'rbf' is another popular choice.\n",
    "# gamma: Kernel coefficient for 'rbf'. 'scale' is a good default.\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC(C=1.0, kernel='linear', random_state=42) # C=1.0 is a standard default\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f'SVM Accuracy: {accuracy_svm:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(svm_model, 'models/svm_model.pkl')\n",
    "print(\"SVM model saved to models/svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Model 3: Fine-Tuning a BERT Transformer\n",
    "\n",
    "For our third model, we will fine-tune a pre-trained BERT model. This involves taking a powerful, general-purpose language model and training it further on our specific sentiment analysis task. This process is more computationally intensive and requires a GPU for reasonable training times.\n",
    "\n",
    "**Note:** Training this model can be slow without a GPU. For this notebook, we will run it for just one epoch on our small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to Modify ---\n",
    "# learning_rate: How much the model's weights are updated during training. A smaller value (e.g., 2e-5) is typical for fine-tuning.\n",
    "# batch_size: The number of samples processed before the model is updated. A smaller batch size requires less memory.\n",
    "# epochs: The number of times the model will see the entire training dataset. More epochs can lead to better performance but also overfitting.\n",
    "\n",
    "# First, we need to map our text labels to integers\n",
    "label_map = {label: i for i, label in enumerate(df['sentiment'].unique())}\n",
    "y_train_int = y_train.map(label_map)\n",
    "y_test_int = y_test.map(label_map)\n",
    "\n",
    "# Tokenize the text data for BERT\n",
    "def tokenize_for_bert(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "train_encodings = tokenize_for_bert(X_train)\n",
    "test_encodings = tokenize_for_bert(X_test)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(y_train_int.tolist()))\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(y_test_int.tolist()))\n",
    "\n",
    "# Initialize the BERT model for sequence classification\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
    "\n",
    "# Set up the data loaders\n",
    "batch_size = 4 # Small batch size for CPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(bert_model.parameters(), lr=5e-5) # AdamW is a common optimizer for transformers\n",
    "\n",
    "# --- Training Loop ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Use GPU if available\n",
    "bert_model.to(device)\n",
    "bert_model.train() # Set the model to training mode\n",
    "\n",
    "epochs = 1 # For demonstration, we only train for one epoch\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss # Get the loss\n",
    "        loss.backward() # Backpropagate the loss\n",
    "        optimizer.step() # Update the weights\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"BERT model training complete.\")\n",
    "\n",
    "# Save the fine-tuned BERT model\n",
    "torch.save(bert_model.state_dict(), 'models/bert_model.pth')\n",
    "print(\"BERT model saved to models/bert_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Evaluation\n",
    "\n",
    "After training, we need to evaluate our models to see how well they perform on unseen data. We will use several metrics:\n",
    "-   **Accuracy:** The proportion of correctly classified tweets.\n",
    "-   **Precision:** The proportion of positive predictions that were actually correct. (TP / (TP + FP))\n",
    "-   **Recall:** The proportion of actual positives that were identified correctly. (TP / (TP + FN))\n",
    "-   **F1-Score:** The harmonic mean of precision and recall, providing a single score that balances both.\n",
    "-   **Confusion Matrix:** A table that visualizes the performance of a classifier, showing the counts of true vs. predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Evaluating the BERT Model\n",
    "\n",
    "First, we need a function to get predictions from our fine-tuned BERT model, as the process is different from scikit-learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions from the BERT model\n",
    "def get_bert_predictions(model, data_loader):\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "# Create a DataLoader for the test set (no shuffling needed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Get predictions for the test set\n",
    "y_pred_bert_int = get_bert_predictions(bert_model, test_loader)\n",
    "\n",
    "# We need to map the integer predictions back to string labels for comparison\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "y_pred_bert = [reverse_label_map[p] for p in y_pred_bert_int]\n",
    "\n",
    "accuracy_bert = accuracy_score(y_test, y_pred_bert)\n",
    "print(f'BERT Model Accuracy: {accuracy_bert:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Comparing All Models\n",
    "\n",
    "Let's compile the key metrics for all three models into a single table for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to store the evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Define the models and their predictions\n",
    "models = {\n",
    "    'Naive Bayes': y_pred_nb,\n",
    "    'SVM': y_pred_svm,\n",
    "    'BERT': y_pred_bert\n",
    "}\n",
    "\n",
    "# Calculate metrics for each model\n",
    "for model_name, y_pred in models.items():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted') # Use 'weighted' for multiclass\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    evaluation_results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "\n",
    "# Create a DataFrame for a clean display\n",
    "results_df = pd.DataFrame(evaluation_results).T # Transpose for better readability\n",
    "display(results_df)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_df.to_json('results/evaluation_results.json', orient='index', indent=4)\n",
    "print(\"Evaluation results saved to results/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Visualizing Performance with Confusion Matrices\n",
    "\n",
    "A confusion matrix provides a detailed breakdown of correct and incorrect classifications for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'Confusion Matrix for {model_name}', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.savefig(f'plots/confusion_matrix_{model_name.replace(\" \", \"_\").lower()}.png') # Save the plot\n",
    "    plt.show()\n",
    "\n",
    "# Get the unique labels for plotting\n",
    "class_labels = sorted(y_test.unique())\n",
    "\n",
    "# Plot the confusion matrix for each model\n",
    "for model_name, y_pred in models.items():\n",
    "    plot_confusion_matrix(y_test, y_pred, model_name, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "The performance of many machine learning models depends heavily on their hyperparameters. Manually tuning them can be tedious. `GridSearchCV` is a technique that automates this process by exhaustively searching through a specified grid of parameter values to find the best combination.\n",
    "\n",
    "Here, we will tune our SVM model to see if we can improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to Modify ---\n",
    "# param_grid: This dictionary defines the hyperparameter space to search. You can add more parameters (e.g., 'gamma') or more values to explore.\n",
    "# cv (Cross-Validation): The number of folds to use for cross-validation. 5 is a common choice. More folds provide a more robust evaluation but increase computation time.\n",
    "# n_jobs: The number of CPU cores to use. Set to -1 to use all available cores, which can significantly speed up the search.\n",
    "\n",
    "# Define the parameter grid for the SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10], # Different values for the regularization parameter\n",
    "    'kernel': ['linear', 'rbf'] # Different kernel types to try\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42), # The model to tune\n",
    "    param_grid,           # The parameters to search\n",
    "    cv=3,                 # Use 3-fold cross-validation\n",
    "    n_jobs=-1,            # Use all available CPU cores\n",
    "    verbose=2             # Show progress\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "print(\"Starting GridSearchCV for SVM...\")\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "print(f\"\\nBest Parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best model found by GridSearchCV on the test set\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best_svm = best_svm.predict(X_test_tfidf)\n",
    "accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)\n",
    "print(f\"Test set accuracy of the best SVM model: {accuracy_best_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualization of Results\n",
    "\n",
    "Visualizations help us understand the data and our model's results more intuitively. We will create word clouds to see the most frequent words for different sentiments and a chart to analyze sentiment trends over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. Word Clouds for Positive and Negative Sentiments\n",
    "\n",
    "Word clouds are a great way to visualize the most prominent words in a body of text. We will generate one for positive tweets and one for negative tweets to see which words are most associated with each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and plot a word cloud\n",
    "def generate_word_cloud(text, title):\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white'\n",
    "    ).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off') # Hide the axes\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.savefig(f'plots/{title.replace(\" \", \"_\").lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Separate text by sentiment\n",
    "positive_text = ' '.join(df[df['sentiment'] == 'positive']['cleaned_tweet'])\n",
    "negative_text = ' '.join(df[df['sentiment'] == 'negative']['cleaned_tweet'])\n",
    "\n",
    "# Generate word clouds\n",
    "if positive_text:\n",
    "    generate_word_cloud(positive_text, 'Most Frequent Words in Positive Tweets')\n",
    "if negative_text:\n",
    "    generate_word_cloud(negative_text, 'Most Frequent Words in Negative Tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. Trend of Sentiments Over Time\n",
    "\n",
    "Analyzing how sentiments change over time can reveal important patterns. Since our sample dataset does not have timestamps, we will create some dummy dates to demonstrate how to create a time-series plot.\n",
    "\n",
    "**Note:** For a real-world application, you would use the actual timestamps from the social media data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy timestamps for demonstration\n",
    "df['timestamp'] = pd.to_datetime(pd.date_range(start='2023-01-01', periods=len(df), freq='h'))\n",
    "\n",
    "# Group by day and sentiment, then count the occurrences\n",
    "sentiment_over_time = df.groupby([df['timestamp'].dt.date, 'sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create the line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sentiment_over_time.plot(kind='line', ax=plt.gca())\n",
    "plt.title('Trend of Sentiments Over Time (Dummy Data)', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Number of Tweets', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/sentiment_trends.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Final Dashboard and Summary\n",
    "\n",
    "This final section provides a dashboard-style summary of our key findings. It combines the most important visualizations and metrics into a single view for a quick, high-level understanding of the project's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with subplots for the dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "fig.suptitle('Sentiment Analysis Project Dashboard', fontsize=24, y=1.02)\n",
    "\n",
    "# 1. Sentiment Distribution (Pie Chart)\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "axes[0, 0].pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=px.colors.qualitative.Pastel)\n",
    "axes[0, 0].set_title('Overall Sentiment Distribution', fontsize=16)\n",
    "\n",
    "# 2. Model Performance Comparison (Bar Chart)\n",
    "results_df['Accuracy'].plot(kind='bar', ax=axes[0, 1], color=px.colors.qualitative.Vivid)\n",
    "axes[0, 1].set_title('Model Accuracy Comparison', fontsize=16)\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "axes[0, 1].set_ylim(0, 1.1)\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[0, 1].text(i, v + 0.02, f'{v:.2f}', ha='center')\n",
    "\n",
    "# 3. Confusion Matrix of the Best Model (SVM)\n",
    "best_model_name = results_df['Accuracy'].idxmax()\n",
    "y_pred_best = models[best_model_name]\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=class_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, ax=axes[1, 0])\n",
    "axes[1, 0].set_title(f'Confusion Matrix for Best Model: {best_model_name}', fontsize=16)\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('True')\n",
    "\n",
    "# 4. Key Insights (Text)\n",
    "axes[1, 1].axis('off') # Hide the axes for the text block\n",
    "insights = f\"\"\"\\n\n",
    "**Key Insights:**\\n\\n\n",
    "1. **Sentiment Balance:** The dataset is fairly balanced,\\n\n",
    "   with {pos_pct:.1f}% positive, {neg_pct:.1f}% negative, and {neu_pct:.1f}% neutral tweets.\\n\\n\n",
    "2. **Best Performing Model:** The **{best_model}** model achieved the\\n\n",
    "   highest accuracy of **{best_acc:.2f}**.\\n\\n\n",
    "3. **Common Words:**\\n\n",
    "   - Positive tweets often contain words like 'love', 'best', 'great'.\\n\n",
    "   - Negative tweets feature words like 'bad', 'hate', 'problem'.\\n\\n\n",
    "4. **Model Behavior:** The confusion matrix shows that the\\n\n",
    "   model is most effective at identifying positive and negative\\n\n",
    "   sentiments but sometimes struggles with neutral tweets.\n",
    "\"\"\".format(\n",
    "    pos_pct=sentiment_counts.get('positive', 0) / len(df) * 100,\n",
    "    neg_pct=sentiment_counts.get('negative', 0) / len(df) * 100,\n",
    "    neu_pct=sentiment_counts.get('neutral', 0) / len(df) * 100,\n",
    "    best_model=best_model_name,\n",
    "    best_acc=results_df['Accuracy'].max()\n",
    ")\n",
    "axes[1, 1].text(0, 0.5, insights, va='center', fontsize=12, wrap=True)\n",
    "\n",
    "# Adjust layout and save the dashboard\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/final_dashboard.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
